{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a fraud detection model with Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates the use of Amazon SageMakwer's implementation of the XGBoost algorithm to train and host a fraud detection model trained on a credit card payment dataset from the DefeatFraud research project. Please see the Data Ackowledgments section at the end of this notebook to learn more about the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up\n",
    "\n",
    "In this section, we will import the necessary libraries and setup variables to work with the dataset and the SageMaker sdk in this notebook.\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "* The IAM role associated with this SageMaker notebook instance.\n",
    "* The S3 bucket used to store the data used to train the model as well as trained model artifacts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the Amazon SageMaker default bucket for the selected AWS region, and then define a key prefix to make sure all objects have share the same prefix for easier discoverability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "sagemaker_iam_role = sagemaker.get_execution_role()\n",
    "\n",
    "prefix = 'sagemaker/DEMO-xgboost-fraud'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will download, extract and read the data set and read the credit card data set into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "wget https://s3-us-west-2.amazonaws.com/sagemaker-e2e-solutions/fraud-detection/creditcardfraud.zip\n",
    "unzip creditcardfraud.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "data = pd.read_csv('creditcard.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Time', 'V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
      "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
      "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount',\n",
      "       'Class'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data.columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class column corresponds to whether or not a transaction is fradulent. We see that the majority of data is non-fraudulant with only $492$ ($.173\\%$) of the data corresponding to fraudulant examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  492\n",
      "Number of non-frauds:  284315\n",
      "Percentage of fradulent data: 0.1727485630620034\n"
     ]
    }
   ],
   "source": [
    "nonfrauds, frauds = data.groupby('Class').size()\n",
    "print('Number of frauds: ', frauds)\n",
    "print('Number of non-frauds: ', nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*frauds/(frauds + nonfrauds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we would typically perform some additional feature engineering on our datset before training a model. Because our dataset has already been processed to some extent, we will skip this for the purposes of this example and move straight to the training. \n",
    "\n",
    "Data processing and feature engineering in SageMaker can be performed in the notebook instance itself, but [Processing Jobs](https://docs.aws.amazon.com/sagemaker/latest/dg/processing-job.html) are recommended for larger workloads. Processing Jobs can be used to process terabytes of data in a SageMaker-managed cluster separate from the instance running your notebook server to run your data processing workloads, such as feature engineering, data validation, model evaluation, and model interpretation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a typical SageMaker workflow, notebooks are only used for prototyping and can be run on relatively inexpensive and less powerful instances, while processing, training and model hosting tasks are run on separate, more powerful SageMaker-managed instances. We could use the xgboost or sklearn libraries to train a model in our notebook instance, but it is recommended to use **training jobs** to take advantage of more powerful compute as well as logging and monitoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload training, validation, and test sets to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training jobs will first download our training and validation datasets from a location in S3, so let's first prepare those csv files and upload them to our S3 bucket. SageMaker's built-in XGBoost algorithm requires that the target variable is in the first column and that the CSV does not have a header record so we will move the Class column from last to first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>...</th>\n",
       "      <th>V20</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>...</td>\n",
       "      <td>0.251412</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069083</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>...</td>\n",
       "      <td>0.524980</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.208038</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408542</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Time        V1        V2        V3        V4        V5        V6  \\\n",
       "0      0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388   \n",
       "1      0   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361   \n",
       "2      0   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499   \n",
       "3      0   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203   \n",
       "4      0   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921   \n",
       "\n",
       "         V7        V8  ...       V20       V21       V22       V23       V24  \\\n",
       "0  0.239599  0.098698  ...  0.251412 -0.018307  0.277838 -0.110474  0.066928   \n",
       "1 -0.078803  0.085102  ... -0.069083 -0.225775 -0.638672  0.101288 -0.339846   \n",
       "2  0.791461  0.247676  ...  0.524980  0.247998  0.771679  0.909412 -0.689281   \n",
       "3  0.237609  0.377436  ... -0.208038 -0.108300  0.005274 -0.190321 -1.175575   \n",
       "4  0.592941 -0.270533  ...  0.408542 -0.009431  0.798278 -0.137458  0.141267   \n",
       "\n",
       "        V25       V26       V27       V28  Amount  \n",
       "0  0.128539 -0.189115  0.133558 -0.021053  149.62  \n",
       "1  0.167170  0.125895 -0.008983  0.014724    2.69  \n",
       "2 -0.327642 -0.139097 -0.055353 -0.059752  378.66  \n",
       "3  0.647376 -0.221929  0.062723  0.061458  123.50  \n",
       "4 -0.206010  0.502292  0.219422  0.215153   69.99  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.concat([data['Class'], data.drop(['Class'], axis=1)], axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To upload the data to S3, we will leverage on the AWS Python SDK (boto3) as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training data location: s3://sagemaker-us-east-1-517094003386/sagemaker/DEMO-xgboost-fraud/train/train.csv\n",
      "Uploaded training data location: s3://sagemaker-us-east-1-517094003386/sagemaker/DEMO-xgboost-fraud/validation/validation.csv\n",
      "Training artifacts will be uploaded to: s3://sagemaker-us-east-1-517094003386/sagemaker/DEMO-xgboost-fraud/output\n"
     ]
    }
   ],
   "source": [
    "train_data, validation_data, test_data = np.split(model_data.sample(frac=1, random_state=1729), \n",
    "                                                  [int(0.7 * len(model_data)), int(0.9 * len(model_data))])\n",
    "train_data.to_csv('train.csv', header=False, index=False)\n",
    "validation_data.to_csv('validation.csv', header=False, index=False)\n",
    "\n",
    "\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train/train.csv')) \\\n",
    "                                .upload_file('train.csv')\n",
    "boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'validation/validation.csv')) \\\n",
    "                                .upload_file('validation.csv')\n",
    "s3_train_data = 's3://{}/{}/train/train.csv'.format(bucket, prefix)\n",
    "s3_validation_data = 's3://{}/{}/validation/validation.csv'.format(bucket, prefix)\n",
    "print('Uploaded training data location: {}'.format(s3_train_data))\n",
    "print('Uploaded training data location: {}'.format(s3_validation_data))\n",
    "\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Training artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a model using SageMaker's built-in XGBoost algorithm\n",
    "\n",
    "The [XGBoost](https://github.com/dmlc/xgboost) (eXtreme Gradient Boosting) is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm that attempts to accurately predict a target variable by combining an ensemble of estimates from a set of simpler and weaker models. The XGBoot algorithm performs well in a variety of machine learning contexts because of its robust handling of a variety of data types, relationships, distributions, and the variety of hyperparameters that you can fine-tune. \n",
    "\n",
    "SageMaker abstracts training with **Estimators** objects which we will create using the SageMaker Python SDK.\n",
    "\n",
    " We can specify a few parameters like what type of training instances we'd like to use and how many, as well as our XGBoost hyperparameters.  A few key hyperparameters are:\n",
    "- `max_depth` controls how deep each tree within the algorithm can be built.  Deeper trees can lead to better fit, but are more computationally expensive and can lead to overfitting.  There is typically some trade-off in model performance that needs to be explored between a large number of shallow trees and a smaller number of deeper trees.\n",
    "- `subsample` controls sampling of the training data.  This technique can help reduce overfitting, but setting it too low can also starve the model of data.\n",
    "- `num_round` controls the number of boosting rounds.  This is essentially the subsequent models that are trained using the residuals of previous iterations.  Again, more rounds should produce a better fit on the training data, but can be computationally expensive or lead to overfitting.\n",
    "- `eta` controls how aggressive each round of boosting is.  Larger values lead to more conservative boosting.\n",
    "- `gamma` controls how aggressively trees are grown.  Larger values lead to more conservative models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"max_depth\": 5,\n",
    "    \"eta\": 0.2,\n",
    "    \"gamma\": 4,\n",
    "    \"min_child_weight\": 6,\n",
    "    \"silent\": 0,\n",
    "    \"subsample\": 0.8,\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"num_round\": 100\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:There is a more up to date SageMaker XGBoost image. To use the newer image, please set 'repo_version'='0.90-1'. For example:\n",
      "\tget_image_uri(region, 'xgboost', '0.90-1').\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI in Elastic Container Regsitry (ECR)\n",
    "container = get_image_uri(boto3.Session().region_name, 'xgboost')\n",
    "\n",
    "xgb_builtin = sagemaker.estimator.Estimator(container,\n",
    "                                    role=sagemaker_iam_role, \n",
    "                                    train_instance_count=1, \n",
    "                                    train_instance_type='ml.m4.xlarge',\n",
    "                                    output_path=output_location,\n",
    "                                    sagemaker_session=session)\n",
    "\n",
    "xgb_builtin.set_hyperparameters(max_depth=5,\n",
    "                        eta=0.2,\n",
    "                        gamma=4,\n",
    "                        min_child_weight=6,\n",
    "                        subsample=0.8,\n",
    "                        silent=0,\n",
    "                        objective='binary:logistic',\n",
    "                        num_round=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Managed Spot Training**: To save on cost, we run the training using managed Spot instances. SageMaker will automatically look to see if any spot instances of the desired type are available for a max time less than the max wait time, and if one is available, run your training job on that. With Managed Spot, customers can benefit from up-to 90% savings in cost.\n",
    "\n",
    "For BYO Containers, customers are responsible for checkpointing models for the spot instances to resume training from, if a spot instance is lost during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, because we're training with the CSV file format, we'll create s3_inputs that our training function can use as a pointer to the files in S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_input_train = sagemaker.s3_input(s3_data='s3://{}/{}/train'.format(bucket, prefix), content_type='csv')\n",
    "s3_input_validation = sagemaker.s3_input(s3_data='s3://{}/{}/validation/'.format(bucket, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created the estimator, we call the `fit` method to run the training job.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_builtin.fit({'train': s3_input_train, 'validation': s3_input_validation}) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selecting the right hyperparameter values to train your model can be difficult, and typically is very time consuming if done manually. The right combination of hyperparameters is dependent on your data and algorithm; some algorithms have many different hyperparameters that can be tweaked; some are very sensitive to the hyperparameter values selected; and most have a non-linear relationship between model fit and hyperparameter values. SageMaker Automatic Model Tuning helps automate the hyperparameter tuning process: it runs multiple training jobs with different hyperparameter combinations to find the set with the best model performance.\n",
    "\n",
    "We begin by specifying the hyperparameters we wish to tune, and the range of values over which to tune each one. We also must specify an objective metric to be optimized: in this use case, we'd like to minimize the validation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "\n",
    "hyperparameter_ranges = {\n",
    "  'max_depth': IntegerParameter(3, 10),\n",
    "  'num_round': IntegerParameter(50, 300),\n",
    "  'eta': ContinuousParameter(0.001, 0.2, scaling_type=\"Logarithmic\"),\n",
    "}\n",
    "\n",
    "objective_metric_name = 'validation:error'\n",
    "objective_type = 'Minimize'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `HyperparameterTuner` object and pass in the estimator we just used to train our model. In this example SageMaker will run 15 training jobs in 3 batches by running 5 jobs in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................!\n"
     ]
    }
   ],
   "source": [
    "from time import gmtime, strftime \n",
    "\n",
    "inputs = {'train': s3_input_train, 'validation': s3_input_validation}\n",
    "\n",
    "tuner = HyperparameterTuner(xgb_builtin,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            max_jobs=15,\n",
    "                            max_parallel_jobs=5,\n",
    "                            objective_type=objective_type)\n",
    "\n",
    "tuning_job_name = \"fraud-detection-{}\".format(strftime(\"%d-%H-%M-%S\", gmtime()))\n",
    "tuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, job_name=tuning_job_name, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the tuning job is finished, we can use the `HyperparameterTuningJobAnalytics` object from the SageMaker Python SDK to list the top 5 tuning jobs with the best performance. Although the results vary from tuning job to tuning job, the best validation loss from the tuning job (under the FinalObjectiveValue column) likely will be substantially lower than the validation loss from the hosted training job above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FinalObjectiveValue</th>\n",
       "      <th>TrainingElapsedTimeSeconds</th>\n",
       "      <th>TrainingEndTime</th>\n",
       "      <th>TrainingJobName</th>\n",
       "      <th>TrainingJobStatus</th>\n",
       "      <th>TrainingStartTime</th>\n",
       "      <th>eta</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>num_round</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000439</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2020-06-18 18:11:06+00:00</td>\n",
       "      <td>fraud-detection-18-17-54-13-015-b0a11ad9</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-06-18 18:08:24+00:00</td>\n",
       "      <td>0.082593</td>\n",
       "      <td>7.0</td>\n",
       "      <td>299.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000456</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2020-06-18 18:09:17+00:00</td>\n",
       "      <td>fraud-detection-18-17-54-13-014-e9397744</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-06-18 18:07:15+00:00</td>\n",
       "      <td>0.093892</td>\n",
       "      <td>3.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000456</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2020-06-18 18:07:42+00:00</td>\n",
       "      <td>fraud-detection-18-17-54-13-011-25b1a036</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-06-18 18:05:44+00:00</td>\n",
       "      <td>0.180291</td>\n",
       "      <td>4.0</td>\n",
       "      <td>267.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000456</td>\n",
       "      <td>183.0</td>\n",
       "      <td>2020-06-18 18:05:39+00:00</td>\n",
       "      <td>fraud-detection-18-17-54-13-010-95273deb</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-06-18 18:02:36+00:00</td>\n",
       "      <td>0.131110</td>\n",
       "      <td>4.0</td>\n",
       "      <td>254.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.000456</td>\n",
       "      <td>122.0</td>\n",
       "      <td>2020-06-18 18:04:31+00:00</td>\n",
       "      <td>fraud-detection-18-17-54-13-009-d80ed742</td>\n",
       "      <td>Completed</td>\n",
       "      <td>2020-06-18 18:02:29+00:00</td>\n",
       "      <td>0.176312</td>\n",
       "      <td>3.0</td>\n",
       "      <td>282.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   FinalObjectiveValue  TrainingElapsedTimeSeconds           TrainingEndTime  \\\n",
       "0             0.000439                       162.0 2020-06-18 18:11:06+00:00   \n",
       "1             0.000456                       122.0 2020-06-18 18:09:17+00:00   \n",
       "4             0.000456                       118.0 2020-06-18 18:07:42+00:00   \n",
       "5             0.000456                       183.0 2020-06-18 18:05:39+00:00   \n",
       "6             0.000456                       122.0 2020-06-18 18:04:31+00:00   \n",
       "\n",
       "                            TrainingJobName TrainingJobStatus  \\\n",
       "0  fraud-detection-18-17-54-13-015-b0a11ad9         Completed   \n",
       "1  fraud-detection-18-17-54-13-014-e9397744         Completed   \n",
       "4  fraud-detection-18-17-54-13-011-25b1a036         Completed   \n",
       "5  fraud-detection-18-17-54-13-010-95273deb         Completed   \n",
       "6  fraud-detection-18-17-54-13-009-d80ed742         Completed   \n",
       "\n",
       "          TrainingStartTime       eta  max_depth  num_round  \n",
       "0 2020-06-18 18:08:24+00:00  0.082593        7.0      299.0  \n",
       "1 2020-06-18 18:07:15+00:00  0.093892        3.0      248.0  \n",
       "4 2020-06-18 18:05:44+00:00  0.180291        4.0      267.0  \n",
       "5 2020-06-18 18:02:36+00:00  0.131110        4.0      254.0  \n",
       "6 2020-06-18 18:02:29+00:00  0.176312        3.0      282.0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuner_metrics = sagemaker.HyperparameterTuningJobAnalytics(tuning_job_name)\n",
    "tuner_metrics.dataframe().sort_values(['FinalObjectiveValue'], ascending=True).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving the model from S3 is very easy: the hosted training estimator you created above stores a reference to the model's location in S3. You simply copy the model from S3 using the estimator's model_data property and unzip it to inspect the contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train a model using the open source XGBoost algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Amazon SageMaker, you can use XGBoost as a built-in algorithm or framework. By using XGBoost as a framework, you have more flexibility and access to more advanced scenarios, such as k-fold cross-validation, because you can customize your own training scripts. The Amazon SageMaker Python SDK provides the XGBoost API as a framework in the same way it provides other framework APIs, such as TensorFlow, MXNet, and PyTorch.\n",
    "\n",
    "Let's see how to train a similiar model using the XGBoost framework instead of the built-in algorithm. First we'll take a look at our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrandom\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mglob\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpickle\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpkl\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mxgboost\u001b[39;49;00m\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mparse_args\u001b[39;49;00m():\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--max_depth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m5\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eta\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mfloat\u001b[39;49;00m, default=\u001b[34m0.05\u001b[39;49;00m)  \u001b[37m# 0.2\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--gamma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m4\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--min_child_weight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m6\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--silent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m0\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--objective\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[33m\"\u001b[39;49;00m\u001b[33mmulti:softmax\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num_class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m15\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--num_round\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m10\u001b[39;49;00m)\n",
      "    \n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--train\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    parser.add_argument(\u001b[33m'\u001b[39;49;00m\u001b[33m--validation\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_CHANNEL_VALIDATION\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "    args = parser.parse_args()\n",
      "\n",
      "    \u001b[34mreturn\u001b[39;49;00m args\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmodel_fn\u001b[39;49;00m(model_dir):\n",
      "    model_file = model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    model = pkl.load(\u001b[36mopen\u001b[39;49;00m(model_file, \u001b[33m'\u001b[39;49;00m\u001b[33mrb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "    \u001b[34mreturn\u001b[39;49;00m model\n",
      "\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32mmain\u001b[39;49;00m():\n",
      "\n",
      "    args = parse_args()\n",
      "    train_files_path, validation_files_path = args.train, args.validation\n",
      "    \n",
      "    train_files_list = glob.glob(train_files_path + \u001b[33m'\u001b[39;49;00m\u001b[33m/*.*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(train_files_list)\n",
      "    \n",
      "    val_files_list = glob.glob(validation_files_path + \u001b[33m'\u001b[39;49;00m\u001b[33m/*.*\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \u001b[34mprint\u001b[39;49;00m(val_files_list)\n",
      "    \n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoading training dataframe...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    df_train = pd.concat(\u001b[36mmap\u001b[39;49;00m(pd.read_csv, train_files_list))\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mLoading validation dataframe...\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    df_val = pd.concat(\u001b[36mmap\u001b[39;49;00m(pd.read_csv, val_files_list))\n",
      "    \u001b[34mprint\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mData loading completed.\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    \n",
      "    y = df_train.Target.values\n",
      "    X =  df_train.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mTarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m).values\n",
      "    val_y = df_val.Target.values\n",
      "    val_X = df_val.drop([\u001b[33m'\u001b[39;49;00m\u001b[33mTarget\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], axis=\u001b[34m1\u001b[39;49;00m).values\n",
      "\n",
      "    dtrain = xgboost.DMatrix(X, label=y)\n",
      "    dval = xgboost.DMatrix(val_X, label=val_y)\n",
      "\n",
      "    watchlist = [(dtrain, \u001b[33m\"\u001b[39;49;00m\u001b[33mtrain\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m), (dval, \u001b[33m\"\u001b[39;49;00m\u001b[33mvalidation\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)]\n",
      "\n",
      "    params = {\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmax_depth\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.max_depth,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33meta\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.eta,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mgamma\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.gamma,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mmin_child_weight\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.min_child_weight,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33msilent\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.silent,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mobjective\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.objective,\n",
      "        \u001b[33m\"\u001b[39;49;00m\u001b[33mnum_class\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: args.num_class\n",
      "    }\n",
      "\n",
      "    bst = xgboost.train(\n",
      "        params=params,\n",
      "        dtrain=dtrain,\n",
      "        evals=watchlist,\n",
      "        num_boost_round=args.num_round)\n",
      "    \n",
      "    model_dir = os.environ.get(\u001b[33m'\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "    pkl.dump(bst, \u001b[36mopen\u001b[39;49;00m(model_dir + \u001b[33m'\u001b[39;49;00m\u001b[33m/model.bin\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mwb\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "    main()\n"
     ]
    }
   ],
   "source": [
    "!pygmentize ./model_scripts/train_xgboost.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The script parses arguments that are passed when the XGBoost Docker container code invokes the script for execution. These arguments represent the hyperparameters that you specify when strarting the training job plus the location of training and validation data.\n",
    "\n",
    "We will create the `XGBoost` estimator object need to add two arguments compared to our original estimator: `entry_point` for the model training script and `source_dir` for the directory containing any files the model needs to train. Otherwise the set-up is exactly the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "entry_point='train_xgboost.py'\n",
    "source_dir='model_scripts/'\n",
    "\n",
    "xgb = XGBoost(\n",
    "            entry_point=entry_point,\n",
    "            source_dir=source_dir,\n",
    "            framework_version=\"0.90-2\",\n",
    "            role=sagemaker_iam_role, \n",
    "            train_instance_count=1, \n",
    "            train_instance_type='ml.m4.xlarge',\n",
    "            output_path=output_location,\n",
    "            sagemaker_session=session,\n",
    "            train_use_spot_instances=True,\n",
    "            train_max_wait=3600,\n",
    "            train_max_run=3600\n",
    "            )\n",
    "\n",
    "xgb.fit({'train': s3_input_train, 'validation': s3_input_validation}) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Host XGBoost Model and Serve Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's deploy the model we just trained to a managed HTTP hosted endpoint and serve predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------"
     ]
    }
   ],
   "source": [
    "xgb.name = 'deployed-xgboost-fraud-prediction'\n",
    "xgb_predictor = xgb.deploy(initial_instance_count = 1, \n",
    "                           instance_type = 'ml.m4.xlarge',\n",
    "                          endpoint_name='deployed-xgboost-fraud-prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate\n",
    "\n",
    "Now that we have a hosted endpoint running, we can make real-time predictions from our model very easily, \n",
    "simply by making an http POST request.  But first, we'll need to setup serializers and deserializers for passing our `test_data` NumPy arrays to the model behind the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.predictor import csv_serializer \n",
    "\n",
    "xgb_predictor.content_type = 'text/csv'\n",
    "xgb_predictor.serializer = csv_serializer\n",
    "xgb_predictor.deserializer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll use a simple function to:\n",
    "1. Loop over our test dataset\n",
    "1. Split it into mini-batches of rows \n",
    "1. Convert those mini-batchs to CSV string payloads\n",
    "1. Retrieve mini-batch predictions by invoking the XGBoost endpoint\n",
    "1. Collect predictions and convert from the CSV output our model provides into a NumPy array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/ipykernel/__main__.py:9: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n"
     ]
    }
   ],
   "source": [
    "def predict(data, rows=500):\n",
    "    split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1))\n",
    "    predictions = ''\n",
    "    for array in split_array:\n",
    "        predictions = ','.join([predictions, xgb_predictor.predict(array).decode('utf-8')])\n",
    "\n",
    "    return np.fromstring(predictions[1:], sep=',')\n",
    "\n",
    "predictions = predict(test_data.as_matrix()[:, 1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways to compare the performance of a machine learning model, but let's start by simply by comparing actual to predicted values. In this case, we're simply predicting whether the customer churned (1) or not (0), which produces a simple confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of frauds:  54\n",
      "Number of non-frauds:  28427\n",
      "Percentage of fradulent data: 0.1896000842667041\n"
     ]
    }
   ],
   "source": [
    "test_nonfrauds, test_frauds = test_data.groupby('Class').size()\n",
    "print('Number of frauds: ', test_frauds)\n",
    "print('Number of non-frauds: ', test_nonfrauds)\n",
    "print('Percentage of fradulent data:', 100.*test_frauds/(test_frauds + test_nonfrauds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>predictions</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>actual</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28424</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "predictions    0.0  1.0\n",
       "actual                 \n",
       "0            28424    3\n",
       "1               12   42"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.round(predictions), rownames=['actual'], colnames=['predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.93\n",
      "recall:  0.78\n"
     ]
    }
   ],
   "source": [
    "#precision: tp / (tp + fp)\n",
    "#recall: tp / (tp + fn)\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "results = precision_recall_fscore_support(test_data.iloc[:, 0],\n",
    "                                         np.round(predictions))\n",
    "print('precision: ', round(results[0][1], 2))\n",
    "print('recall: ', round(results[1][1], 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, due to randomized elements of the algorithm, you results may differ slightly.\n",
    "\n",
    "Of the 54 fraudsters, we've correctly predicted 40 of them (true positives). And, we incorrectly predicted 1 case of fraud (false positive). There are also 14 cases of fraud that the model classified as benign transaction (false negatives) - which can get really expensive.\n",
    "\n",
    "An important point here is that because of the np.round() function above we are using a simple threshold (or cutoff) of 0.5. Our predictions from xgboost come out as continuous values between 0 and 1 and we force them into the binary classes that we began with. So, we should consider adjusting this cutoff. That will almost certainly increase the number of false positives, but it can also be expected to increase the number of true positives and reduce the number of false negatives.\n",
    "\n",
    "To get a rough intuition here, let's look at the continuous values of our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By varying the cutoff threshold, we can trade false positives for false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Class</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28414</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0      0   1\n",
       "Class           \n",
       "0      28414  13\n",
       "1          8  46"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(index=test_data.iloc[:, 0], columns=np.where(predictions > 0.04, 1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision:  0.78\n",
      "recall:  0.85\n"
     ]
    }
   ],
   "source": [
    "results = precision_recall_fscore_support(test_data.iloc[:, 0],\n",
    "                                         np.where(predictions > 0.04, 1, 0))\n",
    "print('precision: ', round(results[0][1], 2))\n",
    "print('recall: ', round(results[1][1], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(predictions)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative cost of errors\n",
    "\n",
    "Any practical binary classification problem is likely to produce a similarly sensitive cutoff. \n",
    "If we put an ML model into production, there are costs associated with the model erroneously assigning false positives and false negatives. Because the choice of the cutoff affects all four of these statistics, we need to consider the relative costs to the business for each of these four outcomes for each prediction.\n",
    "\n",
    "#### Assigning costs\n",
    "\n",
    "What are the costs for our problem fraud detection? The costs, of course, depend on the specific actions that the business takes. Let's make some assumptions here.\n",
    "\n",
    "First, assign the cost of \\$0.00 to both the true negatives (correctly recognized benign transactions) and true positives (correctly recognized fraudulent transactions). Our model essentially correctly identified both situations. One can assign a benefit (i.e. negative cost) to correctly detected fraud, but we are not going to do this here.\n",
    "\n",
    "False negatives are the most problematic, because they represent a fraudulent transactions that slipped through our model. Based on some Internet research (see sources below), we assign a cost of \\$450.00 for each one. This is the cost of false negatives.\n",
    "\n",
    "Finally, False positives are the genuine transactions that our model would block as fraud. This would result in an annoyed customer that might possibly close the credit card account and move to another bank. We assume that it costs a \\$500.00 sign-on bonus to obtain a cr. card customer and that \\5 percent of annoyed customers would defect. \n",
    "\n",
    "Source:\n",
    "\n",
    "https://www.creditcards.com/credit-card-news/credit-card-security-id-theft-fraud-statistics-1276.php\n",
    "https://wallethub.com/edu/cc/credit-debit-card-fraud-statistics/25725/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the optimal cutoff\n",
    "\n",
    "Itâ€™s clear that false negatives are substantially more costly than false positives. We should be minimizing a cost function that looks like this:\n",
    "\n",
    "```txt\n",
    "$450 * FN(C) + $0 * TN(C) + 0.05*$500 * FP(C) + $0 * TP(C)\n",
    "```\n",
    "\n",
    "FN(C) means that the false negative percentage is a function of the cutoff, C, and similar for TN, FP, and TP.  We need to find the cutoff, C, where the result of the expression is smallest.\n",
    "\n",
    "A straightforward way to do this, is to simply run a simulation over a large number of possible cutoffs.  We test 100 possible values in the for loop below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost is minimized near a cutoff of: 0.05\n"
     ]
    }
   ],
   "source": [
    "TN_cost = 0\n",
    "TP_cost = 0\n",
    "FP_cost = 0.05*500 #$cost of losing an annoyed customer (assuming 5% defection and $500 sign-on bonus)\n",
    "FN_cost = 450 # $cost of of letting a fradulent transaction slip through\n",
    "\n",
    "cutoffs = np.arange(0.01, 1, 0.01)\n",
    "costs = []\n",
    "for c in cutoffs:\n",
    "    costs.append(np.sum(np.sum(np.array([[TN_cost, FP_cost], [FN_cost, TP_cost]]) * \n",
    "                               pd.crosstab(index=test_data.iloc[:, 0], \n",
    "                                           columns=np.where(predictions > c, 1, 0)))))\n",
    "\n",
    "costs = np.array(costs)\n",
    "plt.plot(cutoffs, costs)\n",
    "plt.show()\n",
    "print('Cost is minimized near a cutoff of:', cutoffs[np.argmin(costs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up\n",
    "\n",
    "We will leave the prediction endpoint running at the end of this notebook so we can handle incoming event streams. However, don't forget to delete the prediction endpoint when you're done. You can do that at the Amazon SageMaker console in the Endpoints page. Or you can run `xgb_predictor.delete_endpoint()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Data Acknowledgements\n",
    "\n",
    "The dataset used to demonstrated the fraud detection solution has been collected and analysed during a research collaboration of Worldline and the Machine Learning Group (http://mlg.ulb.ac.be) of ULB (UniversitÃ© Libre de Bruxelles) on big data mining and fraud detection. More details on current and past projects on related topics are available on https://www.researchgate.net/project/Fraud-detection-5 and the page of the [DefeatFraud](https://mlg.ulb.ac.be/wordpress/portfolio_page/defeatfraud-assessment-and-validation-of-deep-feature-engineering-and-learning-solutions-for-fraud-detection/) project\n",
    "We cite the following works:\n",
    "* Andrea Dal Pozzolo, Olivier Caelen, Reid A. Johnson and Gianluca Bontempi. Calibrating Probability with Undersampling for Unbalanced Classification. In Symposium on Computational Intelligence and Data Mining (CIDM), IEEE, 2015\n",
    "* Dal Pozzolo, Andrea; Caelen, Olivier; Le Borgne, Yann-Ael; Waterschoot, Serge; Bontempi, Gianluca. Learned lessons in credit card fraud detection from a practitioner perspective, Expert systems with applications,41,10,4915-4928,2014, Pergamon\n",
    "* Dal Pozzolo, Andrea; Boracchi, Giacomo; Caelen, Olivier; Alippi, Cesare; Bontempi, Gianluca. Credit card fraud detection: a realistic modeling and a novel learning strategy, IEEE transactions on neural networks and learning systems,29,8,3784-3797,2018,IEEE\n",
    "* Dal Pozzolo, Andrea Adaptive Machine learning for credit card fraud detection ULB MLG PhD thesis (supervised by G. Bontempi)\n",
    "* Carcillo, Fabrizio; Dal Pozzolo, Andrea; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Mazzer, Yannis; Bontempi, Gianluca. Scarff: a scalable framework for streaming credit card fraud detection with Spark, Information fusion,41, 182-194,2018,Elsevier\n",
    "* Carcillo, Fabrizio; Le Borgne, Yann-AÃ«l; Caelen, Olivier; Bontempi, Gianluca. Streaming active learning strategies for real-life credit card fraud detection: assessment and visualization, International Journal of Data Science and Analytics, 5,4,285-300,2018,Springer International Publishing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
